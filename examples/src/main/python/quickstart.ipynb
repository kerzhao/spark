{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "als.py\t\t\tpagerank.py\t\tstatus_api_demo.py\r\n",
      "avro_inputformat.py\tparquet_inputformat.py\tstreaming\r\n",
      "kmeans.py\t\tpi.py\t\t\ttransitive_closure.py\r\n",
      "logistic_regression.py\tquickstart.ipynb\twordcount.py\r\n",
      "ml\t\t\tsort.py\r\n",
      "mllib\t\t\tsql\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"kmeans.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'#'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\r\n",
      "# Licensed to the Apache Software Foundation (ASF) under one or more\r\n",
      "# contributor license agreements.  See the NOTICE file distributed with\r\n",
      "# this work for additional information regarding copyright ownership.\r\n",
      "# The ASF licenses this file to You under the Apache License, Version 2.0\r\n",
      "# (the \"License\"); you may not use this file except in compliance with\r\n",
      "# the License.  You may obtain a copy of the License at\r\n",
      "#\r\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "#\r\n",
      "# Unless required by applicable law or agreed to in writing, software\r\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "# See the License for the specific language governing permissions and\r\n",
      "# limitations under the License.\r\n",
      "#\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "The K-means algorithm written from scratch against PySpark. In practice,\r\n",
      "one may prefer to use the KMeans algorithm in ML, as shown in\r\n",
      "examples/src/main/python/ml/kmeans_example.py.\r\n",
      "\r\n",
      "This example requires NumPy (http://www.numpy.org/).\r\n",
      "\"\"\"\r\n",
      "from __future__ import print_function\r\n",
      "\r\n",
      "import sys\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "from pyspark.sql import SparkSession\r\n",
      "\r\n",
      "\r\n",
      "def parseVector(line):\r\n",
      "    return np.array([float(x) for x in line.split(' ')])\r\n",
      "\r\n",
      "\r\n",
      "def closestPoint(p, centers):\r\n",
      "    bestIndex = 0\r\n",
      "    closest = float(\"+inf\")\r\n",
      "    for i in range(len(centers)):\r\n",
      "        tempDist = np.sum((p - centers[i]) ** 2)\r\n",
      "        if tempDist < closest:\r\n",
      "            closest = tempDist\r\n",
      "            bestIndex = i\r\n",
      "    return bestIndex\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "\r\n",
      "    if len(sys.argv) != 4:\r\n",
      "        print(\"Usage: kmeans <file> <k> <convergeDist>\", file=sys.stderr)\r\n",
      "        exit(-1)\r\n",
      "\r\n",
      "    print(\"\"\"WARN: This is a naive implementation of KMeans Clustering and is given\r\n",
      "       as an example! Please refer to examples/src/main/python/ml/kmeans_example.py for an\r\n",
      "       example on how to use ML's KMeans implementation.\"\"\", file=sys.stderr)\r\n",
      "\r\n",
      "    spark = SparkSession\\\r\n",
      "        .builder\\\r\n",
      "        .appName(\"PythonKMeans\")\\\r\n",
      "        .getOrCreate()\r\n",
      "\r\n",
      "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\r\n",
      "    data = lines.map(parseVector).cache()\r\n",
      "    K = int(sys.argv[2])\r\n",
      "    convergeDist = float(sys.argv[3])\r\n",
      "\r\n",
      "    kPoints = data.takeSample(False, K, 1)\r\n",
      "    tempDist = 1.0\r\n",
      "\r\n",
      "    while tempDist > convergeDist:\r\n",
      "        closest = data.map(\r\n",
      "            lambda p: (closestPoint(p, kPoints), (p, 1)))\r\n",
      "        pointStats = closest.reduceByKey(\r\n",
      "            lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1]))\r\n",
      "        newPoints = pointStats.map(\r\n",
      "            lambda st: (st[0], st[1][0] / st[1][1])).collect()\r\n",
      "\r\n",
      "        tempDist = sum(np.sum((kPoints[iK] - p) ** 2) for (iK, p) in newPoints)\r\n",
      "\r\n",
      "        for (iK, p) in newPoints:\r\n",
      "            kPoints[iK] = p\r\n",
      "\r\n",
      "    print(\"Final centers: \" + str(kPoints))\r\n",
      "\r\n",
      "    spark.stop()\r\n"
     ]
    }
   ],
   "source": [
    "!cat kmeans.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#',\n",
       " u'# Licensed to the Apache Software Foundation (ASF) under one or more',\n",
       " u'# contributor license agreements.  See the NOTICE file distributed with',\n",
       " u'# this work for additional information regarding copyright ownership.',\n",
       " u'# The ASF licenses this file to You under the Apache License, Version 2.0',\n",
       " u'# (the \"License\"); you may not use this file except in compliance with',\n",
       " u'# the License.  You may obtain a copy of the License at',\n",
       " u'#',\n",
       " u'#    http://www.apache.org/licenses/LICENSE-2.0',\n",
       " u'#',\n",
       " u'# Unless required by applicable law or agreed to in writing, software',\n",
       " u'# distributed under the License is distributed on an \"AS IS\" BASIS,',\n",
       " u'# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.',\n",
       " u'# See the License for the specific language governing permissions and',\n",
       " u'# limitations under the License.',\n",
       " u'#',\n",
       " u'',\n",
       " u'\"\"\"',\n",
       " u'The K-means algorithm written from scratch against PySpark. In practice,',\n",
       " u'one may prefer to use the KMeans algorithm in ML, as shown in',\n",
       " u'examples/src/main/python/ml/kmeans_example.py.',\n",
       " u'',\n",
       " u'This example requires NumPy (http://www.numpy.org/).',\n",
       " u'\"\"\"',\n",
       " u'from __future__ import print_function',\n",
       " u'',\n",
       " u'import sys',\n",
       " u'',\n",
       " u'import numpy as np',\n",
       " u'from pyspark.sql import SparkSession',\n",
       " u'',\n",
       " u'',\n",
       " u'def parseVector(line):',\n",
       " u\"    return np.array([float(x) for x in line.split(' ')])\",\n",
       " u'',\n",
       " u'',\n",
       " u'def closestPoint(p, centers):',\n",
       " u'    bestIndex = 0',\n",
       " u'    closest = float(\"+inf\")',\n",
       " u'    for i in range(len(centers)):',\n",
       " u'        tempDist = np.sum((p - centers[i]) ** 2)',\n",
       " u'        if tempDist < closest:',\n",
       " u'            closest = tempDist',\n",
       " u'            bestIndex = i',\n",
       " u'    return bestIndex',\n",
       " u'',\n",
       " u'',\n",
       " u'if __name__ == \"__main__\":',\n",
       " u'',\n",
       " u'    if len(sys.argv) != 4:',\n",
       " u'        print(\"Usage: kmeans <file> <k> <convergeDist>\", file=sys.stderr)',\n",
       " u'        exit(-1)',\n",
       " u'',\n",
       " u'    print(\"\"\"WARN: This is a naive implementation of KMeans Clustering and is given',\n",
       " u'       as an example! Please refer to examples/src/main/python/ml/kmeans_example.py for an',\n",
       " u'       example on how to use ML\\'s KMeans implementation.\"\"\", file=sys.stderr)',\n",
       " u'',\n",
       " u'    spark = SparkSession\\\\',\n",
       " u'        .builder\\\\',\n",
       " u'        .appName(\"PythonKMeans\")\\\\',\n",
       " u'        .getOrCreate()',\n",
       " u'',\n",
       " u'    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])',\n",
       " u'    data = lines.map(parseVector).cache()',\n",
       " u'    K = int(sys.argv[2])',\n",
       " u'    convergeDist = float(sys.argv[3])',\n",
       " u'',\n",
       " u'    kPoints = data.takeSample(False, K, 1)',\n",
       " u'    tempDist = 1.0',\n",
       " u'',\n",
       " u'    while tempDist > convergeDist:',\n",
       " u'        closest = data.map(',\n",
       " u'            lambda p: (closestPoint(p, kPoints), (p, 1)))',\n",
       " u'        pointStats = closest.reduceByKey(',\n",
       " u'            lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1]))',\n",
       " u'        newPoints = pointStats.map(',\n",
       " u'            lambda st: (st[0], st[1][0] / st[1][1])).collect()',\n",
       " u'',\n",
       " u'        tempDist = sum(np.sum((kPoints[iK] - p) ** 2) for (iK, p) in newPoints)',\n",
       " u'',\n",
       " u'        for (iK, p) in newPoints:',\n",
       " u'            kPoints[iK] = p',\n",
       " u'',\n",
       " u'    print(\"Final centers: \" + str(kPoints))',\n",
       " u'',\n",
       " u'    spark.stop()']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RDD in module pyspark.rdd object:\n",
      "\n",
      "class RDD(__builtin__.object)\n",
      " |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
      " |  Represents an immutable, partitioned collection of elements that can be\n",
      " |  operated on in parallel.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> (rdd + rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  aggregate(self, zeroValue, seqOp, combOp)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given combine functions and a neutral \"zero\n",
      " |      value.\"\n",
      " |      \n",
      " |      The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify C{t2}.\n",
      " |      \n",
      " |      The first function (seqOp) can return a different result type, U, than\n",
      " |      the type of this RDD. Thus, we need one operation for merging a T into\n",
      " |      an U and one operation for merging two U\n",
      " |      \n",
      " |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      " |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (10, 4)\n",
      " |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (0, 0)\n",
      " |  \n",
      " |  aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash>)\n",
      " |      Aggregate the values of each key, using given combine functions and a neutral\n",
      " |      \"zero value\". This function can return a different result type, U, than the type\n",
      " |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
      " |      a U and one operation for merging two U's, The former operation is used for merging\n",
      " |      values within a partition, and the latter is used for merging values between\n",
      " |      partitions. To avoid memory allocation, both of these functions are\n",
      " |      allowed to modify and return their first argument instead of creating a new U.\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persist this RDD with the default storage level (C{MEMORY_ONLY}).\n",
      " |  \n",
      " |  cartesian(self, other)\n",
      " |      Return the Cartesian product of this RDD and another one, that is, the\n",
      " |      RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n",
      " |      C{b} is in C{other}.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2])\n",
      " |      >>> sorted(rdd.cartesian(rdd).collect())\n",
      " |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
      " |  \n",
      " |  checkpoint(self)\n",
      " |      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
      " |      checkpoint directory set with L{SparkContext.setCheckpointDir()} and\n",
      " |      all references to its parent RDDs will be removed. This function must\n",
      " |      be called before any job has been executed on this RDD. It is strongly\n",
      " |      recommended that this RDD is persisted in memory, otherwise saving it\n",
      " |      on a file will require recomputation.\n",
      " |  \n",
      " |  coalesce(self, numPartitions, shuffle=False)\n",
      " |      Return a new RDD that is reduced into `numPartitions` partitions.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
      " |      [[1], [2, 3], [4, 5]]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
      " |      [[1, 2, 3, 4, 5]]\n",
      " |  \n",
      " |  cogroup(self, other, numPartitions=None)\n",
      " |      For each key k in C{self} or C{other}, return a resulting RDD that\n",
      " |      contains a tuple with the list of values for that key in C{self} as\n",
      " |      well as C{other}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n",
      " |      [('a', ([1], [2])), ('b', ([4], []))]\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Return a list that contains all of the elements in this RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |  \n",
      " |  collectAsMap(self)\n",
      " |      Return the key-value pairs in this RDD to the master as a dictionary.\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting data is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
      " |      >>> m[1]\n",
      " |      2\n",
      " |      >>> m[3]\n",
      " |      4\n",
      " |  \n",
      " |  combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash>)\n",
      " |      Generic function to combine the elements for each key using a custom\n",
      " |      set of aggregation functions.\n",
      " |      \n",
      " |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
      " |      type\" C.\n",
      " |      \n",
      " |      Users provide three functions:\n",
      " |      \n",
      " |          - C{createCombiner}, which turns a V into a C (e.g., creates\n",
      " |            a one-element list)\n",
      " |          - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of\n",
      " |            a list)\n",
      " |          - C{mergeCombiners}, to combine two C's into a single one.\n",
      " |      \n",
      " |      In addition, users can control the partitioning of the output RDD.\n",
      " |      \n",
      " |      .. note:: V and C can be different -- for example, one might group an RDD of type\n",
      " |          (Int, Int) into an RDD of type (Int, List[Int]).\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> def add(a, b): return a + str(b)\n",
      " |      >>> sorted(x.combineByKey(str, add, add).collect())\n",
      " |      [('a', '11'), ('b', '1')]\n",
      " |  \n",
      " |  count(self)\n",
      " |      Return the number of elements in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4]).count()\n",
      " |      3\n",
      " |  \n",
      " |  countApprox(self, timeout, confidence=0.95)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Approximate version of count() that returns a potentially incomplete\n",
      " |      result within a timeout, even if not all tasks have finished.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> rdd.countApprox(1000, 1.0)\n",
      " |      1000\n",
      " |  \n",
      " |  countApproxDistinct(self, relativeSD=0.05)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Return approximate number of distinct elements in the RDD.\n",
      " |      \n",
      " |      The algorithm used is based on streamlib's implementation of\n",
      " |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
      " |      of The Art Cardinality Estimation Algorithm\", available here\n",
      " |      <http://dx.doi.org/10.1145/2452376.2452456>`_.\n",
      " |      \n",
      " |      :param relativeSD: Relative accuracy. Smaller values create\n",
      " |                         counters that require more space.\n",
      " |                         It must be greater than 0.000017.\n",
      " |      \n",
      " |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
      " |      >>> 900 < n < 1100\n",
      " |      True\n",
      " |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
      " |      >>> 16 < n < 24\n",
      " |      True\n",
      " |  \n",
      " |  countByKey(self)\n",
      " |      Count the number of elements for each key, and return the result to the\n",
      " |      master as a dictionary.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.countByKey().items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  countByValue(self)\n",
      " |      Return the count of each unique value in this RDD as a dictionary of\n",
      " |      (value, count) pairs.\n",
      " |      \n",
      " |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
      " |      [(1, 2), (2, 3)]\n",
      " |  \n",
      " |  distinct(self, numPartitions=None)\n",
      " |      Return a new RDD containing the distinct elements in this RDD.\n",
      " |      \n",
      " |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  filter(self, f)\n",
      " |      Return a new RDD containing only the elements that satisfy a predicate.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
      " |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  first(self)\n",
      " |      Return the first element in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4]).first()\n",
      " |      2\n",
      " |      >>> sc.parallelize([]).first()\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: RDD is empty\n",
      " |  \n",
      " |  flatMap(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by first applying a function to all elements of this\n",
      " |      RDD, and then flattening the results.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([2, 3, 4])\n",
      " |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
      " |      [1, 1, 1, 2, 2, 3]\n",
      " |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
      " |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
      " |  \n",
      " |  flatMapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a flatMap function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
      " |      >>> def f(x): return x\n",
      " |      >>> x.flatMapValues(f).collect()\n",
      " |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
      " |  \n",
      " |  fold(self, zeroValue, op)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given associative function and a neutral \"zero value.\"\n",
      " |      \n",
      " |      The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify C{t2}.\n",
      " |      \n",
      " |      This behaves somewhat differently from fold operations implemented\n",
      " |      for non-distributed collections in functional languages like Scala.\n",
      " |      This fold operation may be applied to partitions individually, and then\n",
      " |      fold those results into the final result, rather than apply the fold\n",
      " |      to each element sequentially in some defined ordering. For functions\n",
      " |      that are not commutative, the result may differ from that of a fold\n",
      " |      applied to a non-distributed collection.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
      " |      15\n",
      " |  \n",
      " |  foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=<function portable_hash>)\n",
      " |      Merge the values for each key using an associative function \"func\"\n",
      " |      and a neutral \"zeroValue\" which may be added to the result an\n",
      " |      arbitrary number of times, and must not change the result\n",
      " |      (e.g., 0 for addition, or 1 for multiplication.).\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> from operator import add\n",
      " |      >>> sorted(rdd.foldByKey(0, add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies a function to all elements of this RDD.\n",
      " |      \n",
      " |      >>> def f(x): print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies a function to each partition of this RDD.\n",
      " |      \n",
      " |      >>> def f(iterator):\n",
      " |      ...     for x in iterator:\n",
      " |      ...          print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
      " |  \n",
      " |  fullOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of C{self} and C{other}.\n",
      " |      \n",
      " |      For each element (k, v) in C{self}, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in C{other}, or the pair\n",
      " |      (k, (v, None)) if no elements in C{other} have key k.\n",
      " |      \n",
      " |      Similarly, for each element (k, w) in C{other}, the resulting RDD will\n",
      " |      either contain all pairs (k, (v, w)) for v in C{self}, or the pair\n",
      " |      (k, (None, w)) if no elements in C{self} have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
      " |      >>> sorted(x.fullOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
      " |  \n",
      " |  getCheckpointFile(self)\n",
      " |      Gets the name of the file to which this RDD was checkpointed\n",
      " |      \n",
      " |      Not defined if RDD is checkpointed locally.\n",
      " |  \n",
      " |  getNumPartitions(self)\n",
      " |      Returns the number of partitions in RDD\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> rdd.getNumPartitions()\n",
      " |      2\n",
      " |  \n",
      " |  getStorageLevel(self)\n",
      " |      Get the RDD's current storage level.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1,2])\n",
      " |      >>> rdd1.getStorageLevel()\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> print(rdd1.getStorageLevel())\n",
      " |      Serialized 1x Replicated\n",
      " |  \n",
      " |  glom(self)\n",
      " |      Return an RDD created by coalescing all elements within each partition\n",
      " |      into a list.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1, 2], [3, 4]]\n",
      " |  \n",
      " |  groupBy(self, f, numPartitions=None, partitionFunc=<function portable_hash>)\n",
      " |      Return an RDD of grouped items.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
      " |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
      " |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
      " |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
      " |  \n",
      " |  groupByKey(self, numPartitions=None, partitionFunc=<function portable_hash>)\n",
      " |      Group the values for each key in the RDD into a single sequence.\n",
      " |      Hash-partitions the resulting RDD with numPartitions partitions.\n",
      " |      \n",
      " |      .. note:: If you are grouping in order to perform an aggregation (such as a\n",
      " |          sum or average) over each key, using reduceByKey or aggregateByKey will\n",
      " |          provide much better performance.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
      " |      [('a', [1, 1]), ('b', [1])]\n",
      " |  \n",
      " |  groupWith(self, other, *others)\n",
      " |      Alias for cogroup but with support for multiple RDDs.\n",
      " |      \n",
      " |      >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> z = sc.parallelize([(\"b\", 42)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n",
      " |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
      " |  \n",
      " |  histogram(self, buckets)\n",
      " |      Compute a histogram using the provided buckets. The buckets\n",
      " |      are all open to the right except for the last which is closed.\n",
      " |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
      " |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
      " |      and 50 we would have a histogram of 1,0,1.\n",
      " |      \n",
      " |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
      " |      this can be switched from an O(log n) inseration to O(1) per\n",
      " |      element (where n is the number of buckets).\n",
      " |      \n",
      " |      Buckets must be sorted, not contain any duplicates, and have\n",
      " |      at least two elements.\n",
      " |      \n",
      " |      If `buckets` is a number, it will generate buckets which are\n",
      " |      evenly spaced between the minimum and maximum of the RDD. For\n",
      " |      example, if the min value is 0 and the max is 100, given `buckets`\n",
      " |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
      " |      be at least 1. An exception is raised if the RDD contains infinity.\n",
      " |      If the elements in the RDD do not vary (max == min), a single bucket\n",
      " |      will be used.\n",
      " |      \n",
      " |      The return value is a tuple of buckets and histogram.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(51))\n",
      " |      >>> rdd.histogram(2)\n",
      " |      ([0, 25, 50], [25, 26])\n",
      " |      >>> rdd.histogram([0, 5, 25, 50])\n",
      " |      ([0, 5, 25, 50], [5, 20, 26])\n",
      " |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
      " |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
      " |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
      " |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
      " |      (('a', 'b', 'c'), [2, 2])\n",
      " |  \n",
      " |  id(self)\n",
      " |      A unique ID for this RDD (within its SparkContext).\n",
      " |  \n",
      " |  intersection(self, other)\n",
      " |      Return the intersection of this RDD and another one. The output will\n",
      " |      not contain any duplicate elements, even if the input RDDs did.\n",
      " |      \n",
      " |      .. note:: This method performs a shuffle internally.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
      " |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
      " |      >>> rdd1.intersection(rdd2).collect()\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  isCheckpointed(self)\n",
      " |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
      " |  \n",
      " |  isEmpty(self)\n",
      " |      Returns true if and only if the RDD contains no elements at all.\n",
      " |      \n",
      " |      .. note:: an RDD may be empty even when it has at least 1 partition.\n",
      " |      \n",
      " |      >>> sc.parallelize([]).isEmpty()\n",
      " |      True\n",
      " |      >>> sc.parallelize([1]).isEmpty()\n",
      " |      False\n",
      " |  \n",
      " |  isLocallyCheckpointed(self)\n",
      " |      Return whether this RDD is marked for local checkpointing.\n",
      " |      \n",
      " |      Exposed for testing.\n",
      " |  \n",
      " |  join(self, other, numPartitions=None)\n",
      " |      Return an RDD containing all pairs of elements with matching keys in\n",
      " |      C{self} and C{other}.\n",
      " |      \n",
      " |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
      " |      (k, v1) is in C{self} and (k, v2) is in C{other}.\n",
      " |      \n",
      " |      Performs a hash join across the cluster.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
      " |      >>> sorted(x.join(y).collect())\n",
      " |      [('a', (1, 2)), ('a', (1, 3))]\n",
      " |  \n",
      " |  keyBy(self, f)\n",
      " |      Creates tuples of the elements in this RDD by applying C{f}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
      " |      >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
      " |      >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n",
      " |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
      " |  \n",
      " |  keys(self)\n",
      " |      Return an RDD with the keys of each tuple.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
      " |      >>> m.collect()\n",
      " |      [1, 3]\n",
      " |  \n",
      " |  leftOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a left outer join of C{self} and C{other}.\n",
      " |      \n",
      " |      For each element (k, v) in C{self}, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in C{other}, or the pair\n",
      " |      (k, (v, None)) if no elements in C{other} have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(x.leftOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None))]\n",
      " |  \n",
      " |  localCheckpoint(self)\n",
      " |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n",
      " |      \n",
      " |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
      " |      step of replicating the materialized data in a reliable distributed file system. This is\n",
      " |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
      " |      \n",
      " |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
      " |      data is written to ephemeral local storage in the executors instead of to a reliable,\n",
      " |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
      " |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
      " |      \n",
      " |      This is NOT safe to use with dynamic allocation, which removes executors along\n",
      " |      with their cached blocks. If you must use both features, you are advised to set\n",
      " |      L{spark.dynamicAllocation.cachedExecutorIdleTimeout} to a high value.\n",
      " |      \n",
      " |      The checkpoint directory set through L{SparkContext.setCheckpointDir()} is not used.\n",
      " |  \n",
      " |  lookup(self, key)\n",
      " |      Return the list of values in the RDD for key `key`. This operation\n",
      " |      is done efficiently if the RDD has a known partitioner by only\n",
      " |      searching the partition that the key maps to.\n",
      " |      \n",
      " |      >>> l = range(1000)\n",
      " |      >>> rdd = sc.parallelize(zip(l, l), 10)\n",
      " |      >>> rdd.lookup(42)  # slow\n",
      " |      [42]\n",
      " |      >>> sorted = rdd.sortByKey()\n",
      " |      >>> sorted.lookup(42)  # fast\n",
      " |      [42]\n",
      " |      >>> sorted.lookup(1024)\n",
      " |      []\n",
      " |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
      " |      >>> list(rdd2.lookup(('a', 'b'))[0])\n",
      " |      ['c']\n",
      " |  \n",
      " |  map(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each element of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
      " |      [('a', 1), ('b', 1), ('c', 1)]\n",
      " |  \n",
      " |  mapPartitions(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> def f(iterator): yield sum(iterator)\n",
      " |      >>> rdd.mapPartitions(f).collect()\n",
      " |      [3, 7]\n",
      " |  \n",
      " |  mapPartitionsWithIndex(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithIndex(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapPartitionsWithSplit(self, f, preservesPartitioning=False)\n",
      " |      Deprecated: use mapPartitionsWithIndex instead.\n",
      " |      \n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithSplit(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a map function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
      " |      >>> def f(x): return len(x)\n",
      " |      >>> x.mapValues(f).collect()\n",
      " |      [('a', 3), ('b', 1)]\n",
      " |  \n",
      " |  max(self, key=None)\n",
      " |      Find the maximum item in this RDD.\n",
      " |      \n",
      " |      :param key: A function used to generate key for comparing\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.max()\n",
      " |      43.0\n",
      " |      >>> rdd.max(key=str)\n",
      " |      5.0\n",
      " |  \n",
      " |  mean(self)\n",
      " |      Compute the mean of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).mean()\n",
      " |      2.0\n",
      " |  \n",
      " |  meanApprox(self, timeout, confidence=0.95)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Approximate operation to return the mean within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000)) / 1000.0\n",
      " |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  min(self, key=None)\n",
      " |      Find the minimum item in this RDD.\n",
      " |      \n",
      " |      :param key: A function used to generate key for comparing\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.min()\n",
      " |      2.0\n",
      " |      >>> rdd.min(key=str)\n",
      " |      10.0\n",
      " |  \n",
      " |  name(self)\n",
      " |      Return the name of this RDD.\n",
      " |  \n",
      " |  partitionBy(self, numPartitions, partitionFunc=<function portable_hash>)\n",
      " |      Return a copy of the RDD partitioned using the specified partitioner.\n",
      " |      \n",
      " |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
      " |      >>> sets = pairs.partitionBy(2).glom().collect()\n",
      " |      >>> len(set(sets[0]).intersection(set(sets[1])))\n",
      " |      0\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(False, True, False, False, 1))\n",
      " |      Set this RDD's storage level to persist its values across operations\n",
      " |      after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the RDD does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (C{MEMORY_ONLY}).\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> rdd.persist().is_cached\n",
      " |      True\n",
      " |  \n",
      " |  pipe(self, command, env=None, checkCode=False)\n",
      " |      Return an RDD created by piping elements to a forked external process.\n",
      " |      \n",
      " |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
      " |      [u'1', u'2', u'', u'3']\n",
      " |      \n",
      " |      :param checkCode: whether or not to check the return value of the shell command.\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this RDD with the provided weights.\n",
      " |      \n",
      " |      :param weights: weights for splits, will be normalized if they don't sum to 1\n",
      " |      :param seed: random seed\n",
      " |      :return: split RDDs in a list\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(500), 1)\n",
      " |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
      " |      >>> len(rdd1.collect() + rdd2.collect())\n",
      " |      500\n",
      " |      >>> 150 < rdd1.count() < 250\n",
      " |      True\n",
      " |      >>> 250 < rdd2.count() < 350\n",
      " |      True\n",
      " |  \n",
      " |  reduce(self, f)\n",
      " |      Reduces the elements of this RDD using the specified commutative and\n",
      " |      associative binary operator. Currently reduces partitions locally.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
      " |      15\n",
      " |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
      " |      10\n",
      " |      >>> sc.parallelize([]).reduce(add)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: Can not reduce() empty RDD\n",
      " |  \n",
      " |  reduceByKey(self, func, numPartitions=None, partitionFunc=<function portable_hash>)\n",
      " |      Merge the values for each key using an associative and commutative reduce function.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      Output will be partitioned with C{numPartitions} partitions, or\n",
      " |      the default parallelism level if C{numPartitions} is not specified.\n",
      " |      Default partitioner is hash-partition.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKey(add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  reduceByKeyLocally(self, func)\n",
      " |      Merge the values for each key using an associative and commutative reduce function, but\n",
      " |      return the results immediately to the master as a dictionary.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  repartition(self, numPartitions)\n",
      " |      Return a new RDD that has exactly numPartitions partitions.\n",
      " |      \n",
      " |      Can increase or decrease the level of parallelism in this RDD.\n",
      " |      Internally, this uses a shuffle to redistribute data.\n",
      " |      If you are decreasing the number of partitions in this RDD, consider\n",
      " |      using `coalesce`, which can avoid performing a shuffle.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1], [2, 3], [4, 5], [6, 7]]\n",
      " |      >>> len(rdd.repartition(2).glom().collect())\n",
      " |      2\n",
      " |      >>> len(rdd.repartition(10).glom().collect())\n",
      " |      10\n",
      " |  \n",
      " |  repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=<function portable_hash>, ascending=True, keyfunc=<function <lambda>>)\n",
      " |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
      " |      sort records by their keys.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
      " |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, 2)\n",
      " |      >>> rdd2.glom().collect()\n",
      " |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
      " |  \n",
      " |  rightOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of C{self} and C{other}.\n",
      " |      \n",
      " |      For each element (k, w) in C{other}, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
      " |      if no elements in C{self} have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(y.rightOuterJoin(x).collect())\n",
      " |      [('a', (2, 1)), ('b', (None, 4))]\n",
      " |  \n",
      " |  sample(self, withReplacement, fraction, seed=None)\n",
      " |      Return a sampled subset of this RDD.\n",
      " |      \n",
      " |      :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n",
      " |      :param fraction: expected size of the sample as a fraction of this RDD's size\n",
      " |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
      " |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
      " |      :param seed: seed for the random number generator\n",
      " |      \n",
      " |      .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |          count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(100), 4)\n",
      " |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
      " |      True\n",
      " |  \n",
      " |  sampleByKey(self, withReplacement, fractions, seed=None)\n",
      " |      Return a subset of this RDD sampled by key (via stratified sampling).\n",
      " |      Create a sample of this RDD using variable sampling rates for\n",
      " |      different keys as specified by fractions, a key to sampling rate map.\n",
      " |      \n",
      " |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
      " |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
      " |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
      " |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
      " |      True\n",
      " |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
      " |      True\n",
      " |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
      " |      True\n",
      " |  \n",
      " |  sampleStdev(self)\n",
      " |      Compute the sample standard deviation of this RDD's elements (which\n",
      " |      corrects for bias in estimating the standard deviation by dividing by\n",
      " |      N-1 instead of N).\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
      " |      1.0\n",
      " |  \n",
      " |  sampleVariance(self)\n",
      " |      Compute the sample variance of this RDD's elements (which corrects\n",
      " |      for bias in estimating the variance by dividing by N-1 instead of N).\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
      " |      1.0\n",
      " |  \n",
      " |  saveAsHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      L{org.apache.spark.api.python.JavaToWritableConverter}.\n",
      " |      \n",
      " |      :param conf: Hadoop job configuration, passed in as a dict\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |  \n",
      " |  saveAsHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n",
      " |      C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: (None by default)\n",
      " |      :param compressionCodecClass: (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      L{org.apache.spark.api.python.JavaToWritableConverter}.\n",
      " |      \n",
      " |      :param conf: Hadoop job configuration, passed in as a dict\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n",
      " |      C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop job configuration, passed in as a dict (None by default)\n",
      " |  \n",
      " |  saveAsPickleFile(self, path, batchSize=10)\n",
      " |      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
      " |      used is L{pyspark.serializers.PickleSerializer}, default batch size\n",
      " |      is 10.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n",
      " |      ['1', '2', 'rdd', 'spark']\n",
      " |  \n",
      " |  saveAsSequenceFile(self, path, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\n",
      " |      RDD's key and value types. The mechanism is as follows:\n",
      " |      \n",
      " |          1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n",
      " |          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
      " |      \n",
      " |      :param path: path to sequence file\n",
      " |      :param compressionCodecClass: (None by default)\n",
      " |  \n",
      " |  saveAsTextFile(self, path, compressionCodecClass=None)\n",
      " |      Save this RDD as a text file, using string representations of elements.\n",
      " |      \n",
      " |      @param path: path to text file\n",
      " |      @param compressionCodecClass: (None by default) string i.e.\n",
      " |          \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      \n",
      " |      >>> tempFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
      " |      >>> from fileinput import input\n",
      " |      >>> from glob import glob\n",
      " |      >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
      " |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
      " |      \n",
      " |      Empty lines are tolerated when saving to text files.\n",
      " |      \n",
      " |      >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile2.close()\n",
      " |      >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
      " |      >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
      " |      '\\n\\n\\nbar\\nfoo\\n'\n",
      " |      \n",
      " |      Using compressionCodecClass\n",
      " |      \n",
      " |      >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile3.close()\n",
      " |      >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
      " |      >>> from fileinput import input, hook_compressed\n",
      " |      >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n",
      " |      >>> b''.join(result).decode('utf-8')\n",
      " |      u'bar\\nfoo\\n'\n",
      " |  \n",
      " |  setName(self, name)\n",
      " |      Assign a name to this RDD.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1, 2])\n",
      " |      >>> rdd1.setName('RDD1').name()\n",
      " |      u'RDD1'\n",
      " |  \n",
      " |  sortBy(self, keyfunc, ascending=True, numPartitions=None)\n",
      " |      Sorts this RDD by the given keyfunc\n",
      " |      \n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
      " |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |  \n",
      " |  sortByKey(self, ascending=True, numPartitions=None, keyfunc=<function <lambda>>)\n",
      " |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
      " |      # noqa\n",
      " |      \n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey().first()\n",
      " |      ('1', 3)\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
      " |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
      " |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
      " |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
      " |  \n",
      " |  stats(self)\n",
      " |      Return a L{StatCounter} object that captures the mean, variance\n",
      " |      and count of the RDD's elements in one operation.\n",
      " |  \n",
      " |  stdev(self)\n",
      " |      Compute the standard deviation of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).stdev()\n",
      " |      0.816...\n",
      " |  \n",
      " |  subtract(self, other, numPartitions=None)\n",
      " |      Return each value in C{self} that is not contained in C{other}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtract(y).collect())\n",
      " |      [('a', 1), ('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  subtractByKey(self, other, numPartitions=None)\n",
      " |      Return each (key, value) pair in C{self} that has no pair with matching\n",
      " |      key in C{other}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtractByKey(y).collect())\n",
      " |      [('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  sum(self)\n",
      " |      Add up the elements in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
      " |      6.0\n",
      " |  \n",
      " |  sumApprox(self, timeout, confidence=0.95)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Approximate operation to return the sum within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000))\n",
      " |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Take the first num elements of the RDD.\n",
      " |      \n",
      " |      It works by first scanning one partition, and use the results from\n",
      " |      that partition to estimate the number of additional partitions needed\n",
      " |      to satisfy the limit.\n",
      " |      \n",
      " |      Translated from the Scala implementation in RDD#take().\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      " |      [2, 3]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      " |      [2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      " |      [91, 92, 93]\n",
      " |  \n",
      " |  takeOrdered(self, num, key=None)\n",
      " |      Get the N elements from an RDD ordered in ascending order or as\n",
      " |      specified by the optional key function.\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
      " |      [1, 2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
      " |      [10, 9, 7, 6, 5, 4]\n",
      " |  \n",
      " |  takeSample(self, withReplacement, num, seed=None)\n",
      " |      Return a fixed-size sampled subset of this RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(0, 10))\n",
      " |      >>> len(rdd.takeSample(True, 20, 1))\n",
      " |      20\n",
      " |      >>> len(rdd.takeSample(False, 5, 2))\n",
      " |      5\n",
      " |      >>> len(rdd.takeSample(False, 15, 3))\n",
      " |      10\n",
      " |  \n",
      " |  toDF(self, schema=None, sampleRatio=None)\n",
      " |      Converts current :class:`RDD` into a :class:`DataFrame`\n",
      " |      \n",
      " |      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n",
      " |      \n",
      " |      :param schema: a :class:`pyspark.sql.types.StructType` or list of names of columns\n",
      " |      :param samplingRatio: the sample ratio of rows used for inferring\n",
      " |      :return: a DataFrame\n",
      " |      \n",
      " |      >>> rdd.toDF().collect()\n",
      " |      [Row(name=u'Alice', age=1)]\n",
      " |  \n",
      " |  toDebugString(self)\n",
      " |      A description of this RDD and its recursive dependencies for debugging.\n",
      " |  \n",
      " |  toLocalIterator(self)\n",
      " |      Return an iterator that contains all of the elements in this RDD.\n",
      " |      The iterator will consume as much memory as the largest partition in this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(10))\n",
      " |      >>> [x for x in rdd.toLocalIterator()]\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  top(self, num, key=None)\n",
      " |      Get the top N elements from an RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      .. note:: It returns the list sorted in descending order.\n",
      " |      \n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
      " |      [12]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
      " |      [6, 5]\n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
      " |      [4, 3, 2]\n",
      " |  \n",
      " |  treeAggregate(self, zeroValue, seqOp, combOp, depth=2)\n",
      " |      Aggregates the elements of this RDD in a multi-level tree\n",
      " |      pattern.\n",
      " |      \n",
      " |      :param depth: suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeAggregate(0, add, add)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  treeReduce(self, f, depth=2)\n",
      " |      Reduces the elements of this RDD in a multi-level tree pattern.\n",
      " |      \n",
      " |      :param depth: suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeReduce(add)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> rdd.union(rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  unpersist(self)\n",
      " |      Mark the RDD as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |  \n",
      " |  values(self)\n",
      " |      Return an RDD with the values of each tuple.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
      " |      >>> m.collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  variance(self)\n",
      " |      Compute the variance of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).variance()\n",
      " |      0.666...\n",
      " |  \n",
      " |  zip(self, other)\n",
      " |      Zips this RDD with another one, returning key-value pairs with the\n",
      " |      first element in each RDD second element in each RDD, etc. Assumes\n",
      " |      that the two RDDs have the same number of partitions and the same\n",
      " |      number of elements in each partition (e.g. one was made through\n",
      " |      a map on the other).\n",
      " |      \n",
      " |      >>> x = sc.parallelize(range(0,5))\n",
      " |      >>> y = sc.parallelize(range(1000, 1005))\n",
      " |      >>> x.zip(y).collect()\n",
      " |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
      " |  \n",
      " |  zipWithIndex(self)\n",
      " |      Zips this RDD with its element indices.\n",
      " |      \n",
      " |      The ordering is first based on the partition index and then the\n",
      " |      ordering of items within each partition. So the first item in\n",
      " |      the first partition gets index 0, and the last item in the last\n",
      " |      partition receives the largest index.\n",
      " |      \n",
      " |      This method needs to trigger a spark job when this RDD contains\n",
      " |      more than one partitions.\n",
      " |      \n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
      " |  \n",
      " |  zipWithUniqueId(self)\n",
      " |      Zips this RDD with generated unique Long ids.\n",
      " |      \n",
      " |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
      " |      n is the number of partitions. So there may exist gaps, but this\n",
      " |      method won't trigger a spark job, which is different from\n",
      " |      L{zipWithIndex}\n",
      " |      \n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  context\n",
      " |      The L{SparkContext} that this RDD was created on.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "sc.parallelize([3, 2, 1, 3]).aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "            (0, 0) <-- zeroValue\n",
    "\n",
    "[1, 2]                  [3, 4]\n",
    "\n",
    "0 + 1 = 1               0 + 3 = 3\n",
    "0 + 1 = 1               0 + 1 = 1\n",
    "\n",
    "1 + 2 = 3               3 + 4 = 7\n",
    "1 + 1 = 2               1 + 1 = 2       \n",
    "    |                    |\n",
    "    v                    v\n",
    "  (3, 2)                  (7, 2)\n",
    "      \\                    / \n",
    "       \\                  /\n",
    "        \\                /\n",
    "         \\              /\n",
    "          \\            /\n",
    "           \\          / \n",
    "           ------------\n",
    "           |  combOp  |\n",
    "           ------------\n",
    "                |\n",
    "                v\n",
    "             (10, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linesWithSpark = textFile.filter(lambda line: \"Spark\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The K-means algorithm written from scratch against PySpark. In practice,',\n",
       " u'from pyspark.sql import SparkSession',\n",
       " u'    spark = SparkSession\\\\']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textmap = textFile.map(lambda line: len(line.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 12,\n",
       " 10,\n",
       " 9,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 10,\n",
       " 13,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 12,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 12,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textmap.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#',\n",
       " u'# Licensed to the Apache Software Foundation (ASF) under one or more',\n",
       " u'# contributor license agreements.  See the NOTICE file distributed with',\n",
       " u'# this work for additional information regarding copyright ownership.',\n",
       " u'# The ASF licenses this file to You under the Apache License, Version 2.0',\n",
       " u'# (the \"License\"); you may not use this file except in compliance with',\n",
       " u'# the License.  You may obtain a copy of the License at',\n",
       " u'#',\n",
       " u'#    http://www.apache.org/licenses/LICENSE-2.0',\n",
       " u'#',\n",
       " u'# Unless required by applicable law or agreed to in writing, software',\n",
       " u'# distributed under the License is distributed on an \"AS IS\" BASIS,',\n",
       " u'# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.',\n",
       " u'# See the License for the specific language governing permissions and',\n",
       " u'# limitations under the License.',\n",
       " u'#',\n",
       " u'',\n",
       " u'\"\"\"',\n",
       " u'The K-means algorithm written from scratch against PySpark. In practice,',\n",
       " u'one may prefer to use the KMeans algorithm in ML, as shown in',\n",
       " u'examples/src/main/python/ml/kmeans_example.py.',\n",
       " u'',\n",
       " u'This example requires NumPy (http://www.numpy.org/).',\n",
       " u'\"\"\"',\n",
       " u'from __future__ import print_function',\n",
       " u'',\n",
       " u'import sys',\n",
       " u'',\n",
       " u'import numpy as np',\n",
       " u'from pyspark.sql import SparkSession',\n",
       " u'',\n",
       " u'',\n",
       " u'def parseVector(line):',\n",
       " u\"    return np.array([float(x) for x in line.split(' ')])\",\n",
       " u'',\n",
       " u'',\n",
       " u'def closestPoint(p, centers):',\n",
       " u'    bestIndex = 0',\n",
       " u'    closest = float(\"+inf\")',\n",
       " u'    for i in range(len(centers)):',\n",
       " u'        tempDist = np.sum((p - centers[i]) ** 2)',\n",
       " u'        if tempDist < closest:',\n",
       " u'            closest = tempDist',\n",
       " u'            bestIndex = i',\n",
       " u'    return bestIndex',\n",
       " u'',\n",
       " u'',\n",
       " u'if __name__ == \"__main__\":',\n",
       " u'',\n",
       " u'    if len(sys.argv) != 4:',\n",
       " u'        print(\"Usage: kmeans <file> <k> <convergeDist>\", file=sys.stderr)',\n",
       " u'        exit(-1)',\n",
       " u'',\n",
       " u'    print(\"\"\"WARN: This is a naive implementation of KMeans Clustering and is given',\n",
       " u'       as an example! Please refer to examples/src/main/python/ml/kmeans_example.py for an',\n",
       " u'       example on how to use ML\\'s KMeans implementation.\"\"\", file=sys.stderr)',\n",
       " u'',\n",
       " u'    spark = SparkSession\\\\',\n",
       " u'        .builder\\\\',\n",
       " u'        .appName(\"PythonKMeans\")\\\\',\n",
       " u'        .getOrCreate()',\n",
       " u'',\n",
       " u'    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])',\n",
       " u'    data = lines.map(parseVector).cache()',\n",
       " u'    K = int(sys.argv[2])',\n",
       " u'    convergeDist = float(sys.argv[3])',\n",
       " u'',\n",
       " u'    kPoints = data.takeSample(False, K, 1)',\n",
       " u'    tempDist = 1.0',\n",
       " u'',\n",
       " u'    while tempDist > convergeDist:',\n",
       " u'        closest = data.map(',\n",
       " u'            lambda p: (closestPoint(p, kPoints), (p, 1)))',\n",
       " u'        pointStats = closest.reduceByKey(',\n",
       " u'            lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1]))',\n",
       " u'        newPoints = pointStats.map(',\n",
       " u'            lambda st: (st[0], st[1][0] / st[1][1])).collect()',\n",
       " u'',\n",
       " u'        tempDist = sum(np.sum((kPoints[iK] - p) ** 2) for (iK, p) in newPoints)',\n",
       " u'',\n",
       " u'        for (iK, p) in newPoints:',\n",
       " u'            kPoints[iK] = p',\n",
       " u'',\n",
       " u'    print(\"Final centers: \" + str(kPoints))',\n",
       " u'',\n",
       " u'    spark.stop()']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'p2_c2:', 1),\n",
       " (u'practice,', 1),\n",
       " (u'agreed', 1),\n",
       " (u'str(kPoints))', 1),\n",
       " (u'PySpark.', 1),\n",
       " (u'implementation', 1),\n",
       " (u'newPoints', 1),\n",
       " (u'distributed', 3),\n",
       " (u'1)))', 1),\n",
       " (u'CONDITIONS', 1),\n",
       " (u'WARRANTIES', 1),\n",
       " (u'pointStats.map(', 1),\n",
       " (u'return', 2),\n",
       " (u'centers):', 1),\n",
       " (u'==', 1),\n",
       " (u'Please', 1),\n",
       " (u'not', 1),\n",
       " (u'writing,', 1),\n",
       " (u'examples/src/main/python/ml/kmeans_example.py', 1),\n",
       " (u'st[1][1])).collect()', 1),\n",
       " (u'regarding', 1),\n",
       " (u'r[0])', 1),\n",
       " (u'spark.stop()', 1),\n",
       " (u'kPoints),', 1),\n",
       " (u'specific', 1),\n",
       " (u'p:', 1),\n",
       " (u'NumPy', 1),\n",
       " (u'Clustering', 1),\n",
       " (u'convergeDist:', 1),\n",
       " (u'st[1][0]', 1),\n",
       " (u'range(len(centers)):', 1),\n",
       " (u'licenses', 1),\n",
       " (u'len(sys.argv)', 1),\n",
       " (u'license', 1),\n",
       " (u'#', 16),\n",
       " (u'closest', 3),\n",
       " (u'for', 7),\n",
       " (u'+', 3),\n",
       " (u'while', 1),\n",
       " (u'/', 1),\n",
       " (u'written', 1),\n",
       " (u'K', 1),\n",
       " (u'limitations', 1),\n",
       " (u'This', 2),\n",
       " (u'np.sum((p', 1),\n",
       " (u'given', 1),\n",
       " (u'governing', 1),\n",
       " (u'Unless', 1),\n",
       " (u'spark', 1),\n",
       " (u'print_function', 1),\n",
       " (u'copyright', 1),\n",
       " (u'language', 1),\n",
       " (u'against', 1),\n",
       " (u'data.takeSample(False,', 1),\n",
       " (u\"')])\", 1),\n",
       " (u'implied.', 1),\n",
       " (u'centers:', 1),\n",
       " (u'(iK,', 2),\n",
       " (u'newPoints)', 1),\n",
       " (u'prefer', 1),\n",
       " (u'contributor', 1),\n",
       " (u'!=', 1),\n",
       " (u'<file>', 1),\n",
       " (u'use', 3),\n",
       " (u'from', 3),\n",
       " (u'(http://www.numpy.org/).', 1),\n",
       " (u'BASIS,', 1),\n",
       " (u'SparkSession\\\\', 1),\n",
       " (u'\"License\");', 1),\n",
       " (u'numpy', 1),\n",
       " (u'st:', 1),\n",
       " (u'.builder\\\\', 1),\n",
       " (u'naive', 1),\n",
       " (u'Apache', 2),\n",
       " (u'agreements.', 1),\n",
       " (u'with', 2),\n",
       " (u'\"__main__\":', 1),\n",
       " (u'<k>', 1),\n",
       " (u'(ASF)', 1),\n",
       " (u'tempDist', 6),\n",
       " (u'this', 3),\n",
       " (u'See', 2),\n",
       " (u'.getOrCreate()', 1),\n",
       " (u'p1_c1,', 1),\n",
       " (u'1)', 1),\n",
       " (u'def', 2),\n",
       " (u'and', 2),\n",
       " (u'NOTICE', 1),\n",
       " (u'1.0', 1),\n",
       " (u'is', 3),\n",
       " (u'as', 3),\n",
       " (u'r:', 1),\n",
       " (u'file', 3),\n",
       " (u'You', 2),\n",
       " (u'4:', 1),\n",
       " (u'float(\"+inf\")', 1),\n",
       " (u'-', 2),\n",
       " (u'ML,', 1),\n",
       " (u'(st[0],', 1),\n",
       " (u'closestPoint(p,', 1),\n",
       " (u'np', 1),\n",
       " (u'you', 1),\n",
       " (u'=', 17),\n",
       " (u'License', 3),\n",
       " (u'may', 3),\n",
       " (u'shown', 1),\n",
       " (u'sys', 1),\n",
       " (u'**', 2),\n",
       " (u'The', 2),\n",
       " (u'data', 1),\n",
       " (u'a', 2),\n",
       " (u'algorithm', 2),\n",
       " (u'i', 2),\n",
       " (u'lines', 1),\n",
       " (u'the', 10),\n",
       " (u'requires', 1),\n",
       " (u\"ML's\", 1),\n",
       " (u'scratch', 1),\n",
       " (u'obtain', 1),\n",
       " (u\"line.split('\", 1),\n",
       " (u'Foundation', 1),\n",
       " (u'copy', 1),\n",
       " (u'how', 1),\n",
       " (u'pyspark.sql', 1),\n",
       " (u'except', 1),\n",
       " (u'0', 1),\n",
       " (u'under', 4),\n",
       " (u'2)', 2),\n",
       " (u'(closestPoint(p,', 1),\n",
       " (u'express', 1),\n",
       " (u'float(sys.argv[3])', 1),\n",
       " (u'>', 1),\n",
       " (u'implementation.\"\"\",', 1),\n",
       " (u'K,', 1),\n",
       " (u'permissions', 1),\n",
       " (u'bestIndex', 3),\n",
       " (u'pointStats', 1),\n",
       " (u'OF', 1),\n",
       " (u'applicable', 1),\n",
       " (u'p', 1),\n",
       " (u'either', 1),\n",
       " (u'x', 1),\n",
       " (u'print(\"\"\"WARN:', 1),\n",
       " (u'OR', 1),\n",
       " (u'(p1_c1[0]', 1),\n",
       " (u'(the', 1),\n",
       " (u'closest.reduceByKey(', 1),\n",
       " (u'KMeans', 3),\n",
       " (u'\"\"\"', 2),\n",
       " (u'Version', 1),\n",
       " (u'kPoints[iK]', 1),\n",
       " (u'2.0', 1),\n",
       " (u'K-means', 1),\n",
       " (u'import', 4),\n",
       " (u'refer', 1),\n",
       " (u'example!', 1),\n",
       " (u'WITHOUT', 1),\n",
       " (u'IS\"', 1),\n",
       " (u'__name__', 1),\n",
       " (u'by', 1),\n",
       " (u'sum(np.sum((kPoints[iK]', 1),\n",
       " (u'on', 2),\n",
       " (u'p)', 3),\n",
       " (u'\"AS', 1),\n",
       " (u'of', 2),\n",
       " (u'compliance', 1),\n",
       " (u'convergeDist', 1),\n",
       " (u'int(sys.argv[2])', 1),\n",
       " (u'or', 3),\n",
       " (u'lambda', 3),\n",
       " (u'__future__', 1),\n",
       " (u'kPoints', 1),\n",
       " (u'kmeans', 1),\n",
       " (u'one', 2),\n",
       " (u'newPoints:', 1),\n",
       " (u'examples/src/main/python/ml/kmeans_example.py.', 1),\n",
       " (u'\"', 1),\n",
       " (u'additional', 1),\n",
       " (u'to', 6),\n",
       " (u'data.map(', 1),\n",
       " (u'ASF', 1),\n",
       " (u'centers[i])', 1),\n",
       " (u'print(\"Usage:', 1),\n",
       " (u'(p,', 1),\n",
       " (u'ANY', 1),\n",
       " (u'KIND,', 1),\n",
       " (u'Software', 1),\n",
       " (u'work', 1),\n",
       " (u'parseVector(line):', 1),\n",
       " (u'exit(-1)', 1),\n",
       " (u'np.array([float(x)', 1),\n",
       " (u'ownership.', 1),\n",
       " (u'more', 1),\n",
       " (u'example', 2),\n",
       " (u'http://www.apache.org/licenses/LICENSE-2.0', 1),\n",
       " (u'an', 3),\n",
       " (u'.appName(\"PythonKMeans\")\\\\', 1),\n",
       " (u'at', 1),\n",
       " (u'in', 8),\n",
       " (u'file=sys.stderr)', 2),\n",
       " (u'if', 3),\n",
       " (u'information', 1),\n",
       " (u'License.', 2),\n",
       " (u'License,', 1),\n",
       " (u'lines.map(parseVector).cache()', 1),\n",
       " (u'spark.read.text(sys.argv[1]).rdd.map(lambda', 1),\n",
       " (u'closest:', 1),\n",
       " (u'<convergeDist>\",', 1),\n",
       " (u'SparkSession', 1),\n",
       " (u'p2_c2[1]))', 1),\n",
       " (u'print(\"Final', 1),\n",
       " (u'law', 1),\n",
       " (u'p2_c2[0],', 1),\n",
       " (u'required', 1),\n",
       " (u'Licensed', 1),\n",
       " (u'In', 1),\n",
       " (u'p1_c1[1]', 1),\n",
       " (u'software', 1),\n",
       " (u'<', 1)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SparkContext in module pyspark.context:\n",
      "\n",
      "class SparkContext(__builtin__.object)\n",
      " |  Main entry point for Spark functionality. A SparkContext represents the\n",
      " |  connection to a Spark cluster, and can be used to create L{RDD} and\n",
      " |  broadcast variables on that cluster.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n",
      " |  \n",
      " |  __exit__(self, type, value, trace)\n",
      " |      Enable 'with SparkContext(...) as sc: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the context on exit of the with block.\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)\n",
      " |      Create a new SparkContext. At least the master and app name should be set,\n",
      " |      either through the named parameters here or through C{conf}.\n",
      " |      \n",
      " |      :param master: Cluster URL to connect to\n",
      " |             (e.g. mesos://host:port, spark://host:port, local[4]).\n",
      " |      :param appName: A name for your job, to display on the cluster web UI.\n",
      " |      :param sparkHome: Location where Spark is installed on cluster nodes.\n",
      " |      :param pyFiles: Collection of .zip or .py files to send to the cluster\n",
      " |             and add to PYTHONPATH.  These can be paths on the local file\n",
      " |             system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
      " |      :param environment: A dictionary of environment variables to set on\n",
      " |             worker nodes.\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. Set 1 to disable batching, 0 to automatically choose\n",
      " |             the batch size based on object sizes, or -1 to use an unlimited\n",
      " |             batch size\n",
      " |      :param serializer: The serializer for RDDs.\n",
      " |      :param conf: A L{SparkConf} object setting Spark properties.\n",
      " |      :param gateway: Use an existing gateway and JVM, otherwise a new JVM\n",
      " |             will be instantiated.\n",
      " |      :param jsc: The JavaSparkContext instance (optional).\n",
      " |      :param profiler_cls: A class of custom Profiler used to do profiling\n",
      " |             (default is pyspark.profiler.BasicProfiler).\n",
      " |      \n",
      " |      \n",
      " |      >>> from pyspark.context import SparkContext\n",
      " |      >>> sc = SparkContext('local', 'test')\n",
      " |      \n",
      " |      >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError:...\n",
      " |  \n",
      " |  accumulator(self, value, accum_param=None)\n",
      " |      Create an L{Accumulator} with the given initial value, using a given\n",
      " |      L{AccumulatorParam} helper object to define how to add values of the\n",
      " |      data type if provided. Default AccumulatorParams are used for integers\n",
      " |      and floating-point numbers if you do not provide one. For other types,\n",
      " |      a custom AccumulatorParam can be used.\n",
      " |  \n",
      " |  addFile(self, path, recursive=False)\n",
      " |      Add a file to be downloaded with this Spark job on every node.\n",
      " |      The C{path} passed can be either a local file, a file in HDFS\n",
      " |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
      " |      FTP URI.\n",
      " |      \n",
      " |      To access the file in Spark jobs, use\n",
      " |      L{SparkFiles.get(fileName)<pyspark.files.SparkFiles.get>} with the\n",
      " |      filename to find its download location.\n",
      " |      \n",
      " |      A directory can be given if the recursive option is set to True.\n",
      " |      Currently directories are only supported for Hadoop-supported filesystems.\n",
      " |      \n",
      " |      >>> from pyspark import SparkFiles\n",
      " |      >>> path = os.path.join(tempdir, \"test.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"100\")\n",
      " |      >>> sc.addFile(path)\n",
      " |      >>> def func(iterator):\n",
      " |      ...    with open(SparkFiles.get(\"test.txt\")) as testFile:\n",
      " |      ...        fileVal = int(testFile.readline())\n",
      " |      ...        return [x * fileVal for x in iterator]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
      " |      [100, 200, 300, 400]\n",
      " |  \n",
      " |  addPyFile(self, path)\n",
      " |      Add a .py or .zip dependency for all tasks to be executed on this\n",
      " |      SparkContext in the future.  The C{path} passed can be either a local\n",
      " |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n",
      " |      HTTP, HTTPS or FTP URI.\n",
      " |  \n",
      " |  binaryFiles(self, path, minPartitions=None)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Read a directory of binary files from HDFS, a local file system\n",
      " |      (available on all nodes), or any Hadoop-supported file system URI\n",
      " |      as a byte array. Each file is read as a single record and returned\n",
      " |      in a key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      .. note:: Small files are preferred, large file is also allowable, but\n",
      " |          may cause bad performance.\n",
      " |  \n",
      " |  binaryRecords(self, path, recordLength)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Load data from a flat binary file, assuming each record is a set of numbers\n",
      " |      with the specified numerical format (see ByteBuffer), and the number of\n",
      " |      bytes per record is constant.\n",
      " |      \n",
      " |      :param path: Directory to the input data files\n",
      " |      :param recordLength: The length at which to split the records\n",
      " |  \n",
      " |  broadcast(self, value)\n",
      " |      Broadcast a read-only variable to the cluster, returning a\n",
      " |      L{Broadcast<pyspark.broadcast.Broadcast>}\n",
      " |      object for reading it in distributed functions. The variable will\n",
      " |      be sent to each cluster only once.\n",
      " |  \n",
      " |  cancelAllJobs(self)\n",
      " |      Cancel all jobs that have been scheduled or are running.\n",
      " |  \n",
      " |  cancelJobGroup(self, groupId)\n",
      " |      Cancel active jobs for the specified group. See L{SparkContext.setJobGroup}\n",
      " |      for more information.\n",
      " |  \n",
      " |  dump_profiles(self, path)\n",
      " |      Dump the profile stats into directory `path`\n",
      " |  \n",
      " |  emptyRDD(self)\n",
      " |      Create an RDD that has no partitions or elements.\n",
      " |  \n",
      " |  getConf(self)\n",
      " |  \n",
      " |  getLocalProperty(self, key)\n",
      " |      Get a local property set in this thread, or null if it is missing. See\n",
      " |      L{setLocalProperty}\n",
      " |  \n",
      " |  hadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  hadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  parallelize(self, c, numSlices=None)\n",
      " |      Distribute a local Python collection to form an RDD. Using xrange\n",
      " |      is recommended if the input represents a range for performance.\n",
      " |      \n",
      " |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      " |      [[0], [2], [3], [4], [6]]\n",
      " |      >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
      " |      [[], [0], [], [2], [4]]\n",
      " |  \n",
      " |  pickleFile(self, name, minPartitions=None)\n",
      " |      Load an RDD previously saved using L{RDD.saveAsPickleFile} method.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 3).collect())\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numSlices=None)\n",
      " |      Create a new RDD of int containing elements from `start` to `end`\n",
      " |      (exclusive), increased by `step` every element. Can be called the same\n",
      " |      way as python's built-in range() function. If called with a single argument,\n",
      " |      the argument is interpreted as `end`, and `start` is set to 0.\n",
      " |      \n",
      " |      :param start: the start value\n",
      " |      :param end: the end value (exclusive)\n",
      " |      :param step: the incremental step (default: 1)\n",
      " |      :param numSlices: the number of partitions of the new RDD\n",
      " |      :return: An RDD of int\n",
      " |      \n",
      " |      >>> sc.range(5).collect()\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> sc.range(2, 4).collect()\n",
      " |      [2, 3]\n",
      " |      >>> sc.range(1, 7, 2).collect()\n",
      " |      [1, 3, 5]\n",
      " |  \n",
      " |  runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False)\n",
      " |      Executes the given partitionFunc on the specified set of partitions,\n",
      " |      returning the result as an array of elements.\n",
      " |      \n",
      " |      If 'partitions' is not specified, this will run over all partitions.\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n",
      " |      [0, 1, 4, 9, 16, 25]\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n",
      " |      [0, 1, 16, 25]\n",
      " |  \n",
      " |  sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)\n",
      " |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is as follows:\n",
      " |      \n",
      " |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n",
      " |             and value Writable classes\n",
      " |          2. Serialization is attempted via Pyrolite pickling\n",
      " |          3. If this fails, the fallback is to call 'toString' on each key and value\n",
      " |          4. C{PickleSerializer} is used to deserialize pickled objects on the Python side\n",
      " |      \n",
      " |      :param path: path to sequncefile\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter:\n",
      " |      :param valueConverter:\n",
      " |      :param minSplits: minimum splits in dataset\n",
      " |             (default min(2, sc.defaultParallelism))\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  setCheckpointDir(self, dirName)\n",
      " |      Set the directory under which RDDs are going to be checkpointed. The\n",
      " |      directory must be a HDFS path if running on a cluster.\n",
      " |  \n",
      " |  setJobGroup(self, groupId, description, interruptOnCancel=False)\n",
      " |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n",
      " |      different value or cleared.\n",
      " |      \n",
      " |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n",
      " |      Application programmers can use this method to group all those jobs together and give a\n",
      " |      group description. Once set, the Spark web UI will associate such jobs with this group.\n",
      " |      \n",
      " |      The application can use L{SparkContext.cancelJobGroup} to cancel all\n",
      " |      running jobs in this group.\n",
      " |      \n",
      " |      >>> import threading\n",
      " |      >>> from time import sleep\n",
      " |      >>> result = \"Not Set\"\n",
      " |      >>> lock = threading.Lock()\n",
      " |      >>> def map_func(x):\n",
      " |      ...     sleep(100)\n",
      " |      ...     raise Exception(\"Task should have been cancelled\")\n",
      " |      >>> def start_job(x):\n",
      " |      ...     global result\n",
      " |      ...     try:\n",
      " |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
      " |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
      " |      ...     except Exception as e:\n",
      " |      ...         result = \"Cancelled\"\n",
      " |      ...     lock.release()\n",
      " |      >>> def stop_job():\n",
      " |      ...     sleep(5)\n",
      " |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n",
      " |      >>> supress = lock.acquire()\n",
      " |      >>> supress = threading.Thread(target=start_job, args=(10,)).start()\n",
      " |      >>> supress = threading.Thread(target=stop_job).start()\n",
      " |      >>> supress = lock.acquire()\n",
      " |      >>> print(result)\n",
      " |      Cancelled\n",
      " |      \n",
      " |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n",
      " |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n",
      " |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n",
      " |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n",
      " |  \n",
      " |  setLocalProperty(self, key, value)\n",
      " |      Set a local property that affects jobs submitted from this thread, such as the\n",
      " |      Spark fair scheduler pool.\n",
      " |  \n",
      " |  setLogLevel(self, logLevel)\n",
      " |      Control our logLevel. This overrides any user-defined log settings.\n",
      " |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
      " |  \n",
      " |  show_profiles(self)\n",
      " |      Print the profile stats to stdout\n",
      " |  \n",
      " |  sparkUser(self)\n",
      " |      Get SPARK_USER for user who is running SparkContext.\n",
      " |  \n",
      " |  statusTracker(self)\n",
      " |      Return :class:`StatusTracker` object\n",
      " |  \n",
      " |  stop(self)\n",
      " |      Shut down the SparkContext.\n",
      " |  \n",
      " |  textFile(self, name, minPartitions=None, use_unicode=True)\n",
      " |      Read a text file from HDFS, a local file system (available on all\n",
      " |      nodes), or any Hadoop-supported file system URI, and return it as an\n",
      " |      RDD of Strings.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"sample-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello world!\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      [u'Hello world!']\n",
      " |  \n",
      " |  union(self, rdds)\n",
      " |      Build the union of a list of RDDs.\n",
      " |      \n",
      " |      This supports unions() of RDDs with different serialized formats,\n",
      " |      although this forces them to be reserialized using the default\n",
      " |      serializer:\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"union-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      [u'Hello']\n",
      " |      >>> parallelized = sc.parallelize([\"World!\"])\n",
      " |      >>> sorted(sc.union([textFile, parallelized]).collect())\n",
      " |      [u'Hello', 'World!']\n",
      " |  \n",
      " |  wholeTextFiles(self, path, minPartitions=None, use_unicode=True)\n",
      " |      Read a directory of text files from HDFS, a local file system\n",
      " |      (available on all nodes), or any  Hadoop-supported file system\n",
      " |      URI. Each file is read as a single record and returned in a\n",
      " |      key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      For example, if you have the following files::\n",
      " |      \n",
      " |        hdfs://a-hdfs-path/part-00000\n",
      " |        hdfs://a-hdfs-path/part-00001\n",
      " |        ...\n",
      " |        hdfs://a-hdfs-path/part-nnnnn\n",
      " |      \n",
      " |      Do C{rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")},\n",
      " |      then C{rdd} contains::\n",
      " |      \n",
      " |        (a-hdfs-path/part-00000, its content)\n",
      " |        (a-hdfs-path/part-00001, its content)\n",
      " |        ...\n",
      " |        (a-hdfs-path/part-nnnnn, its content)\n",
      " |      \n",
      " |      .. note:: Small files are preferred, as each file will be loaded\n",
      " |          fully in memory.\n",
      " |      \n",
      " |      >>> dirPath = os.path.join(tempdir, \"files\")\n",
      " |      >>> os.mkdir(dirPath)\n",
      " |      >>> with open(os.path.join(dirPath, \"1.txt\"), \"w\") as file1:\n",
      " |      ...    _ = file1.write(\"1\")\n",
      " |      >>> with open(os.path.join(dirPath, \"2.txt\"), \"w\") as file2:\n",
      " |      ...    _ = file2.write(\"2\")\n",
      " |      >>> textFiles = sc.wholeTextFiles(dirPath)\n",
      " |      >>> sorted(textFiles.collect())\n",
      " |      [(u'.../1.txt', u'1'), (u'.../2.txt', u'2')]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getOrCreate(cls, conf=None) from __builtin__.type\n",
      " |      Get or instantiate a SparkContext and register it as a singleton object.\n",
      " |      \n",
      " |      :param conf: SparkConf (optional)\n",
      " |  \n",
      " |  setSystemProperty(cls, key, value) from __builtin__.type\n",
      " |      Set a Java system property, such as spark.executor.memory. This must\n",
      " |      must be invoked before instantiating SparkContext.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  applicationId\n",
      " |      A unique identifier for the Spark application.\n",
      " |      Its format depends on the scheduler implementation.\n",
      " |      \n",
      " |      * in case of local spark app something like 'local-1433865536131'\n",
      " |      * in case of YARN something like 'application_1433865536131_34483'\n",
      " |      \n",
      " |      >>> sc.applicationId  # doctest: +ELLIPSIS\n",
      " |      u'local-...'\n",
      " |  \n",
      " |  defaultMinPartitions\n",
      " |      Default min number of partitions for Hadoop RDDs when not given by user\n",
      " |  \n",
      " |  defaultParallelism\n",
      " |      Default level of parallelism to use when not given by user (e.g. for\n",
      " |      reduce tasks)\n",
      " |  \n",
      " |  startTime\n",
      " |      Return the epoch time when the Spark Context was started.\n",
      " |  \n",
      " |  uiWebUrl\n",
      " |      Return the URL of the SparkUI instance started by this SparkContext\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "help(SparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregate.ipynb\t\tmllib\t\t\tsql\r\n",
      "als.py\t\t\tpagerank.py\t\tstatus_api_demo.py\r\n",
      "avro_inputformat.py\tparquet_inputformat.py\tstreaming\r\n",
      "kmeans.py\t\tpi.py\t\t\ttransitive_closure.py\r\n",
      "logistic_regression.py\tquickstart.ipynb\twordcount.py\r\n",
      "ml\t\t\tsort.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 43, lines with b: 12\n"
     ]
    }
   ],
   "source": [
    "\"\"\"SimpleApp.py\"\"\"\n",
    "from pyspark import SparkContext\n",
    "\n",
    "logFile = \"kmeans.py\"  # Should be some file on your system\n",
    "#sc = SparkContext(\"local\", \"Simple App\")\n",
    "sc.master = 'local'\n",
    "sc.appName = 'Simple App'\n",
    "logData = sc.textFile(logFile).cache()\n",
    "\n",
    "numAs = logData.filter(lambda s: 'a' in s).count()\n",
    "numBs = logData.filter(lambda s: 'b' in s).count()\n",
    "\n",
    "print(\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]0;IPython: main/python\u0007Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "17/05/08 05:45:13 INFO SparkContext: Running Spark version 2.1.0\n",
      "17/05/08 05:45:13 INFO SecurityManager: Changing view acls to: ubuntu\n",
      "17/05/08 05:45:13 INFO SecurityManager: Changing modify acls to: ubuntu\n",
      "17/05/08 05:45:13 INFO SecurityManager: Changing view acls groups to: \n",
      "17/05/08 05:45:13 INFO SecurityManager: Changing modify acls groups to: \n",
      "17/05/08 05:45:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "17/05/08 05:45:14 INFO Utils: Successfully started service 'sparkDriver' on port 45844.\n",
      "17/05/08 05:45:14 INFO SparkEnv: Registering MapOutputTracker\n",
      "17/05/08 05:45:14 INFO SparkEnv: Registering BlockManagerMaster\n",
      "17/05/08 05:45:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "17/05/08 05:45:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "17/05/08 05:45:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-010b0105-c440-4371-9080-6c1d21f3f4c7\n",
      "17/05/08 05:45:14 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "17/05/08 05:45:14 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "17/05/08 05:45:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "17/05/08 05:45:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "17/05/08 05:45:14 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "17/05/08 05:45:14 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.31.9.93:4042\n",
      "17/05/08 05:45:14 INFO SparkContext: Added file file:/home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py at file:/home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py with timestamp 1494222314616\n",
      "17/05/08 05:45:14 INFO Utils: Copying /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py to /tmp/spark-15f7c623-4f7a-4540-9513-40fe8ca822cb/userFiles-9838a81b-db7d-4988-836a-3df5ba425ed8/SimpleApp.py\n",
      "17/05/08 05:45:14 INFO Executor: Starting executor ID driver on host localhost\n",
      "17/05/08 05:45:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44554.\n",
      "17/05/08 05:45:14 INFO NettyBlockTransferService: Server created on 172.31.9.93:44554\n",
      "17/05/08 05:45:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "17/05/08 05:45:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.31.9.93, 44554, None)\n",
      "17/05/08 05:45:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.9.93:44554 with 366.3 MB RAM, BlockManagerId(driver, 172.31.9.93, 44554, None)\n",
      "17/05/08 05:45:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.31.9.93, 44554, None)\n",
      "17/05/08 05:45:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.31.9.93, 44554, None)\n",
      "17/05/08 05:45:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 229.7 KB, free 366.1 MB)\n",
      "17/05/08 05:45:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 21.8 KB, free 366.1 MB)\n",
      "17/05/08 05:45:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.31.9.93:44554 (size: 21.8 KB, free: 366.3 MB)\n",
      "17/05/08 05:45:15 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "17/05/08 05:45:15 INFO FileInputFormat: Total input paths to process : 1\n",
      "17/05/08 05:45:15 INFO SparkContext: Starting job: count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:10\n",
      "17/05/08 05:45:15 INFO DAGScheduler: Got job 0 (count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:10) with 1 output partitions\n",
      "17/05/08 05:45:15 INFO DAGScheduler: Final stage: ResultStage 0 (count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:10)\n",
      "17/05/08 05:45:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "17/05/08 05:45:15 INFO DAGScheduler: Missing parents: List()\n",
      "17/05/08 05:45:15 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:10), which has no missing parents\n",
      "17/05/08 05:45:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.7 KB, free 366.0 MB)\n",
      "17/05/08 05:45:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.1 KB, free 366.0 MB)\n",
      "17/05/08 05:45:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.31.9.93:44554 (size: 4.1 KB, free: 366.3 MB)\n",
      "17/05/08 05:45:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996\n",
      "17/05/08 05:45:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:10)\n",
      "17/05/08 05:45:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "17/05/08 05:45:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6183 bytes)\n",
      "17/05/08 05:45:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "17/05/08 05:45:15 INFO Executor: Fetching file:/home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py with timestamp 1494222314616\n",
      "17/05/08 05:45:15 INFO Utils: /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py has been previously copied to /tmp/spark-15f7c623-4f7a-4540-9513-40fe8ca822cb/userFiles-9838a81b-db7d-4988-836a-3df5ba425ed8/SimpleApp.py\n",
      "17/05/08 05:45:16 INFO HadoopRDD: Input split: file:/home/ubuntu/gtest/spark/examples/src/main/python/kmeans.py:0+2773\n",
      "17/05/08 05:45:16 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "17/05/08 05:45:16 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "17/05/08 05:45:16 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "17/05/08 05:45:16 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "17/05/08 05:45:16 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "17/05/08 05:45:16 INFO MemoryStore: Block rdd_1_0 stored as bytes in memory (estimated size 2.0 KB, free 366.0 MB)\n",
      "17/05/08 05:45:16 INFO BlockManagerInfo: Added rdd_1_0 in memory on 172.31.9.93:44554 (size: 2.0 KB, free: 366.3 MB)\n",
      "17/05/08 05:45:16 INFO PythonRunner: Times: total = 272, boot = 261, init = 11, finish = 0\n",
      "17/05/08 05:45:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2256 bytes result sent to driver\n",
      "17/05/08 05:45:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 669 ms on localhost (executor driver) (1/1)\n",
      "17/05/08 05:45:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "17/05/08 05:45:16 INFO DAGScheduler: ResultStage 0 (count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:10) finished in 0.684 s\n",
      "17/05/08 05:45:16 INFO DAGScheduler: Job 0 finished: count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:10, took 0.861259 s\n",
      "17/05/08 05:45:16 INFO SparkContext: Starting job: count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:11\n",
      "17/05/08 05:45:16 INFO DAGScheduler: Got job 1 (count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:11) with 1 output partitions\n",
      "17/05/08 05:45:16 INFO DAGScheduler: Final stage: ResultStage 1 (count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:11)\n",
      "17/05/08 05:45:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "17/05/08 05:45:16 INFO DAGScheduler: Missing parents: List()\n",
      "17/05/08 05:45:16 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[3] at count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:11), which has no missing parents\n",
      "17/05/08 05:45:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 6.7 KB, free 366.0 MB)\n",
      "17/05/08 05:45:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.1 KB, free 366.0 MB)\n",
      "17/05/08 05:45:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.31.9.93:44554 (size: 4.1 KB, free: 366.3 MB)\n",
      "17/05/08 05:45:16 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996\n",
      "17/05/08 05:45:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[3] at count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:11)\n",
      "17/05/08 05:45:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "17/05/08 05:45:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 6183 bytes)\n",
      "17/05/08 05:45:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "17/05/08 05:45:16 INFO BlockManager: Found block rdd_1_0 locally\n",
      "17/05/08 05:45:16 INFO PythonRunner: Times: total = 38, boot = -101, init = 138, finish = 1\n",
      "17/05/08 05:45:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1731 bytes result sent to driver\n",
      "17/05/08 05:45:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 59 ms on localhost (executor driver) (1/1)\n",
      "17/05/08 05:45:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "17/05/08 05:45:16 INFO DAGScheduler: ResultStage 1 (count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:11) finished in 0.058 s\n",
      "17/05/08 05:45:16 INFO DAGScheduler: Job 1 finished: count at /home/ubuntu/gtest/spark/examples/src/main/python/SimpleApp.py:11, took 0.075047 s\n",
      "Lines with a: 43, lines with b: 12\n",
      "17/05/08 05:45:16 INFO SparkUI: Stopped Spark web UI at http://172.31.9.93:4042\n",
      "17/05/08 05:45:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "17/05/08 05:45:16 INFO MemoryStore: MemoryStore cleared\n",
      "17/05/08 05:45:16 INFO BlockManager: BlockManager stopped\n",
      "17/05/08 05:45:16 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "17/05/08 05:45:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "17/05/08 05:45:16 INFO SparkContext: Successfully stopped SparkContext\n",
      "17/05/08 05:45:17 INFO ShutdownHookManager: Shutdown hook called\n",
      "17/05/08 05:45:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-15f7c623-4f7a-4540-9513-40fe8ca822cb/pyspark-c102ede6-14e0-45cb-b74c-4a407103d85b\n",
      "17/05/08 05:45:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-15f7c623-4f7a-4540-9513-40fe8ca822cb\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --master=local[4] SimpleApp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]0;IPython: main/python\u0007Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "17/05/08 05:47:53 INFO SparkContext: Running Spark version 2.1.0\n",
      "17/05/08 05:47:53 INFO SecurityManager: Changing view acls to: ubuntu\n",
      "17/05/08 05:47:53 INFO SecurityManager: Changing modify acls to: ubuntu\n",
      "17/05/08 05:47:53 INFO SecurityManager: Changing view acls groups to: \n",
      "17/05/08 05:47:53 INFO SecurityManager: Changing modify acls groups to: \n",
      "17/05/08 05:47:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "17/05/08 05:47:54 INFO Utils: Successfully started service 'sparkDriver' on port 34665.\n",
      "17/05/08 05:47:54 INFO SparkEnv: Registering MapOutputTracker\n",
      "17/05/08 05:47:54 INFO SparkEnv: Registering BlockManagerMaster\n",
      "17/05/08 05:47:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "17/05/08 05:47:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "17/05/08 05:47:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3de7a578-8d3e-442d-9ae0-7a731bacf4fc\n",
      "17/05/08 05:47:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "17/05/08 05:47:54 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "17/05/08 05:47:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "17/05/08 05:47:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "17/05/08 05:47:54 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "17/05/08 05:47:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.31.9.93:4042\n",
      "17/05/08 05:47:54 INFO SparkContext: Added file file:/home/ubuntu/gtest/spark/examples/src/main/python/pi.py at file:/home/ubuntu/gtest/spark/examples/src/main/python/pi.py with timestamp 1494222474720\n",
      "17/05/08 05:47:54 INFO Utils: Copying /home/ubuntu/gtest/spark/examples/src/main/python/pi.py to /tmp/spark-b44d4a6c-1d02-4971-9163-ac30a6f5bc65/userFiles-eee8e8b2-3736-47b6-9c7a-1186c43c00c5/pi.py\n",
      "17/05/08 05:47:54 INFO Executor: Starting executor ID driver on host localhost\n",
      "17/05/08 05:47:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33510.\n",
      "17/05/08 05:47:54 INFO NettyBlockTransferService: Server created on 172.31.9.93:33510\n",
      "17/05/08 05:47:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "17/05/08 05:47:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.31.9.93, 33510, None)\n",
      "17/05/08 05:47:54 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.9.93:33510 with 366.3 MB RAM, BlockManagerId(driver, 172.31.9.93, 33510, None)\n",
      "17/05/08 05:47:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.31.9.93, 33510, None)\n",
      "17/05/08 05:47:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.31.9.93, 33510, None)\n",
      "17/05/08 05:47:55 INFO SharedState: Warehouse path is 'file:/home/ubuntu/gtest/spark/examples/src/main/python/spark-warehouse'.\n",
      "17/05/08 05:47:55 INFO SparkContext: Starting job: reduce at /home/ubuntu/gtest/spark/examples/src/main/python/pi.py:43\n",
      "17/05/08 05:47:55 INFO DAGScheduler: Got job 0 (reduce at /home/ubuntu/gtest/spark/examples/src/main/python/pi.py:43) with 2 output partitions\n",
      "17/05/08 05:47:55 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at /home/ubuntu/gtest/spark/examples/src/main/python/pi.py:43)\n",
      "17/05/08 05:47:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "17/05/08 05:47:55 INFO DAGScheduler: Missing parents: List()\n",
      "17/05/08 05:47:55 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at reduce at /home/ubuntu/gtest/spark/examples/src/main/python/pi.py:43), which has no missing parents\n",
      "17/05/08 05:47:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.5 KB, free 366.3 MB)\n",
      "17/05/08 05:47:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 366.3 MB)\n",
      "17/05/08 05:47:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.31.9.93:33510 (size: 3.0 KB, free: 366.3 MB)\n",
      "17/05/08 05:47:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996\n",
      "17/05/08 05:47:55 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at /home/ubuntu/gtest/spark/examples/src/main/python/pi.py:43)\n",
      "17/05/08 05:47:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n",
      "17/05/08 05:47:56 WARN TaskSetManager: Stage 0 contains a task of very large size (369 KB). The maximum recommended task size is 100 KB.\n",
      "17/05/08 05:47:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 378493 bytes)\n",
      "17/05/08 05:47:56 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 506296 bytes)\n",
      "17/05/08 05:47:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "17/05/08 05:47:56 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "17/05/08 05:47:56 INFO Executor: Fetching file:/home/ubuntu/gtest/spark/examples/src/main/python/pi.py with timestamp 1494222474720\n",
      "17/05/08 05:47:56 INFO Utils: /home/ubuntu/gtest/spark/examples/src/main/python/pi.py has been previously copied to /tmp/spark-b44d4a6c-1d02-4971-9163-ac30a6f5bc65/userFiles-eee8e8b2-3736-47b6-9c7a-1186c43c00c5/pi.py\n",
      "17/05/08 05:47:56 INFO PythonRunner: Times: total = 365, boot = 277, init = 14, finish = 74\n",
      "17/05/08 05:47:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1574 bytes result sent to driver\n",
      "17/05/08 05:47:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 606 ms on localhost (executor driver) (1/2)\n",
      "17/05/08 05:47:56 INFO PythonRunner: Times: total = 428, boot = 280, init = 17, finish = 131\n",
      "17/05/08 05:47:56 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1574 bytes result sent to driver\n",
      "17/05/08 05:47:56 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 565 ms on localhost (executor driver) (2/2)\n",
      "17/05/08 05:47:56 INFO DAGScheduler: ResultStage 0 (reduce at /home/ubuntu/gtest/spark/examples/src/main/python/pi.py:43) finished in 0.651 s\n",
      "17/05/08 05:47:56 INFO DAGScheduler: Job 0 finished: reduce at /home/ubuntu/gtest/spark/examples/src/main/python/pi.py:43, took 1.028299 s\n",
      "17/05/08 05:47:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "Pi is roughly 3.132340\n",
      "17/05/08 05:47:56 INFO SparkUI: Stopped Spark web UI at http://172.31.9.93:4042\n",
      "17/05/08 05:47:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "17/05/08 05:47:56 INFO MemoryStore: MemoryStore cleared\n",
      "17/05/08 05:47:56 INFO BlockManager: BlockManager stopped\n",
      "17/05/08 05:47:56 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "17/05/08 05:47:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "17/05/08 05:47:56 INFO SparkContext: Successfully stopped SparkContext\n",
      "17/05/08 05:47:57 INFO ShutdownHookManager: Shutdown hook called\n",
      "17/05/08 05:47:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-b44d4a6c-1d02-4971-9163-ac30a6f5bc65/pyspark-7586d2d7-6ffb-4a40-9691-0c8ef6ea1617\n",
      "17/05/08 05:47:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-b44d4a6c-1d02-4971-9163-ac30a6f5bc65\n"
     ]
    }
   ],
   "source": [
    "!spark-submit pi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "#\r\n",
      "# Licensed to the Apache Software Foundation (ASF) under one or more\r\n",
      "# contributor license agreements.  See the NOTICE file distributed with\r\n",
      "# this work for additional information regarding copyright ownership.\r\n",
      "# The ASF licenses this file to You under the Apache License, Version 2.0\r\n",
      "# (the \"License\"); you may not use this file except in compliance with\r\n",
      "# the License.  You may obtain a copy of the License at\r\n",
      "#\r\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "#\r\n",
      "# Unless required by applicable law or agreed to in writing, software\r\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "# See the License for the specific language governing permissions and\r\n",
      "# limitations under the License.\r\n",
      "#\r\n",
      "\r\n",
      "import sys\r\n",
      "from random import random\r\n",
      "from operator import add\r\n",
      "\r\n",
      "from pyspark.sql import SparkSession\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    \"\"\"\r\n",
      "        Usage: pi [partitions]\r\n",
      "    \"\"\"\r\n",
      "    spark = SparkSession\\\r\n",
      "        .builder\\\r\n",
      "        .appName(\"PythonPi\")\\\r\n",
      "        .getOrCreate()\r\n",
      "\r\n",
      "    partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\r\n",
      "    n = 100000 * partitions\r\n",
      "\r\n",
      "    def f(_):\r\n",
      "        x = random() * 2 - 1\r\n",
      "        y = random() * 2 - 1\r\n",
      "        return 1 if x ** 2 + y ** 2 <= 1 else 0\r\n",
      "\r\n",
      "    count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\r\n",
      "    print(\"Pi is roughly %f\" % (4.0 * count / n))\r\n",
      "\r\n",
      "    spark.stop()\r\n"
     ]
    }
   ],
   "source": [
    "!cat pi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
